## FlashInfer: Optimized Kernels for LLM Serving

As LLMs moved from research to production, the inference workload – especially serving user queries with minimal latency – became a central focus. **FlashInfer** (Ye et al., 2024) emerged as a specialized library targeting _end-to-end LLM inference optimization_, with an emphasis on the self-attention bottleneck in decoder models. The motivation, as described by its authors, was that while dense matrix multiplication (GEMM) optimizations are well-studied, less attention had been paid to optimizing the attention mechanism in the context of LLM serving

[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=efficacy%20of%20operators%20within%20Transformers,source%20library%20for)

- In generation, the attention pattern differs from training: models use a _KV cache_ of past keys and values, and each new token’s attention only covers the prior context (which grows over time). FlashInfer analyzes this process in three stages – **prefill, decode, and append** – to pinpoint bottlenecks and introduce tailored optimizations for each stage​

[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=During%20the%20prefill%20stage%2C%20attention,and%20queries%20of%20the%20appended)

- _Prefill stage:_ the initial stage where the model processes the user’s prompt (context) all at once, attending over the prompt tokens. Here attention is performed between the query and a freshly built (empty) KV cache of similar length.
    
- _Decode stage:_ the auto-regressive generation stage, where the model produces tokens one by one. For each new token, attention occurs between the single-token query and the accumulated KV cache of all previous tokens​ [flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=During%20the%20prefill%20stage%2C%20attention,and%20queries%20of%20the%20appended). This is a crucial loop where latency per token must be minimal.
    
- _Append stage:_ after a token is generated (or a batch of tokens in batched decoding), the new token’s key/value are **appended** to the KV cache for future use ​[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=During%20the%20prefill%20stage%2C%20attention,and%20queries%20of%20the%20appended). Managing this efficiently (especially for batched requests of different lengths) is important for throughput.

**Technical innovations:** FlashInfer provides a **comprehensive suite of custom GPU kernels** covering all these stages with high efficiency. Its design is modular and _format-aware_: it supports multiple KV cache memory layouts (contiguous, “ragged” variable length, and even a paged layout used by systems like vLLM)​ [flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=1,for%20long) For each scenario, FlashInfer includes optimized kernels – for example, a specialized kernel for the _decode_ stage that assumes a single query token and streams through the KV cache with minimal overhead, and another for _prefill_ that handles a full attention matrix. These kernels are implemented to be **batch-aware** as well: FlashInfer has variants for single-request inference versus batched inference (serving many prompts concurrently)[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=1,in%20an%20impressive%20up%20to). 

A key contribution is how it handles _shared-prefix batching_: when multiple requests share a long common prefix (e.g. many users prompt with the same initial context), FlashInfer’s “cascading” technique reuses the work across those requests to avoid redundant computations, yielding massive speedups in extreme cases​ [flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=2.%20Optimized%20Shared,Notably). In one benchmark, exploiting a shared 32k-token prompt for a batch of 256 requests, the FlashInfer approach was up to **31× faster** than a naive implementation (using vLLM’s paging attention as baseline)​[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=2.%20Optimized%20Shared,Notably) 

Another innovation is support for **grouped-query attention and quantized keys/values**. Many modern LLMs employ _Grouped-Query Attention (GQA)_ – a variant where instead of each of N heads having separate key/value sets, heads are grouped to share keys and values (this reduces memory size of KV cache and bandwidth)​ [flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation). FlashInfer provides kernels optimized for GQA and for common quantization of KV caches (e.g. storing cache in 16-bit or 8-bit), including support for fused position encoding (RoPE) during attention computation​ [flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation). By optimizing memory access patterns for these compressed representations, FlashInfer can achieve **2–3× speedups**even when the KV cache is quantized or grouped, compared to standard implementations​[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation).

Under the hood, FlashInfer uses _just-in-time (JIT) code generation_ to adapt its kernels to specific model shapes and deployment needs​[ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=present%20FlashInfer%3A%20a%20customizable%20and,end%20evaluations). The library exposes a template where at compile-time you can specify the attention variant, data types, head count, etc., and it will JIT compile an optimized kernel specialized for that case​ [ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=present%20FlashInfer%3A%20a%20customizable%20and,end%20evaluations). This is done with a lightweight C++ template and CUDA code approach (the library is dependency-free and header-only for easy integration​[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=FlashInfer%20has%20been%20developed%20by,FlashInfer%20has%20several%20unique%20advantages)). The result is that it can flexibly accommodate different model architectures or hardware specifics, similar in spirit to how _TVM or TensorRT_ might tune an operator.

Additionally, FlashInfer implements a custom **load-balanced scheduling**for batched decoding​ [ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=format%20and%20composable%20formats%20to,art%20LLM%20serving). In dynamic serving, some requests may finish earlier than others or have varying lengths, which can lead to load imbalance. FlashInfer’s scheduler tries to distribute work evenly across CUDA streams/blocks while remaining compatible with **CUDA Graphs** (which require static launch shapes)​ [ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=format%20and%20composable%20formats%20to,art%20LLM%20serving) – a non-trivial balancing act.

**Performance benchmarks:** According to the authors’ evaluation, FlashInfer delivers significant gains over existing inference solutions. At the kernel level, across diverse scenarios (single token decode, multi-token prefill, various batch sizes), it outperforms baseline kernels from vendor libraries and other open-source projects. End-to-end, when integrated into serving systems, FlashInfer achieved **29–69% lower inter-token latency** (the time per output token) compared to state-of-the-art compiler-based backends on a standard LLM serving benchmark​
[ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=vLLM%20and%20MLC,LLM%20serving%20with%20parallel%20generation). 

For long-context inference (e.g. prompts that are thousands of tokens long), it reduced latency by ~28–30%​ [ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=across%20diverse%20inference%20scenarios%3A%20compared,LLM%20serving%20with%20parallel%20generation), showing that its optimizations on memory access and batching pay off for extended sequences. In scenarios with _parallel generation_ (serving multiple requests concurrently, leveraging batching), it offered around 13–17% throughput improvement versus the best alternatives​ [ar5iv.org](https://ar5iv.org/abs/2501.01005#:~:text=across%20diverse%20inference%20scenarios%3A%20compared,LLM%20serving%20with%20parallel%20generation). These numbers indicate that FlashInfer doesn’t just optimize one narrow case but provides across-the-board efficiency improvements. Notably, FlashInfer’s gains are complementary to other techniques like quantization – e.g., one can quantize an LLM and still use FlashInfer to accelerate the attention, stacking the benefits.

**Comparisons and adoption:** FlashInfer can be seen as part of a new wave of high-performance inference runtimes. Other frameworks like **DeepSpeed-Inference, FasterTransformer, and TensorRT-LLM** also provide optimized kernels (and support things like quantization and tensor parallelism). 
What sets FlashInfer apart is its focus and coverage of the _attention_ operator specifically, with fine-grained optimizations for different caching patterns and batch behaviors. It essentially generalizes “FlashAttention-style” thinking to the whole inference process. 

The library has already been integrated with several systems: for example, the authors report that FlashInfer was adopted as the CUDA attention backend in **MLC-LLM** (an AI compiler stack for LLMs), and it’s used in the **vLLM** and **SGLang** serving frameworks​
[flashinfer.ai](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=FlashInfer%20has%20been%20adopted%20by,or%20%2010%20creating%20an). Because it offers both PyTorch APIs and a C++ API, developers can use it either for experimenting in Python or link it into custom serving infrastructure. 

In sum, FlashInfer represents the state-of-the-art in _specialized LLM inference kernels_, pushing latency down by addressing the unique characteristics of decoder attention workloads. It complements other optimizations (it can work alongside model quantization, weight pruning, etc.), and is likely to inspire further research into inference-specific kernel fusion and scheduling for Transformers.

[Transformer Batching](https://le.qun.ch/en/blog/2023/05/13/transformer-batching/)

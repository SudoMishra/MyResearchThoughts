## ‚ö° [[FlashInfer]]
- **Summary**: Efficient attention engine for LLM inference.
- **Key Features**:
  - Handles KV-cache storage heterogeneity via block-sparse format.
  - Customizable attention templates via JIT compilation.
  - Load-balanced scheduling compatible with CUDA Graphs.
- **Link**: [FlashInfer - arXiv](https://arxiv.org/abs/2501.01005?utm_source=chatgpt.com)

---

## ‚ö° [[FlashAttention-3]]
- **Summary**: Improved version leveraging asynchronous execution and FP8.
- **Key Features**:
  - Uses warp-specialization and Tensor Memory Accelerator (TMA).
  - Interleaves softmax and matrix multiplication for better overlap.
  - Achieves up to 740 TFLOPs/s on Hopper GPUs.
- **Link**: [FlashAttention-3 - arXiv](https://arxiv.org/pdf/2407.08608?utm_source=chatgpt.com)

---

## üß™ [[Speculative Decoding Techniques]] (e.g., EMS-SD)
- **Summary**: Accelerates LLM inference by guessing tokens and verifying.
- **Key Features**:
  - Multi-sample decoding without additional memory overhead.
  - Efficient handling of dynamic acceptance lengths.
- **Link**: [EMS-SD - arXiv](https://arxiv.org/pdf/2405.07542?utm_source=chatgpt.com)

---

## üöÄ [[Spin (Speculative Inference)]]
- **Summary**: Serving system for LLMs based on speculative decoding.
- **Key Features**:
  - Uses heterogeneous speculative models.
  - ML-based speculative model selection.
  - GPU pipelining between speculation and verification.
- **Link**: [Spin - arXiv](https://arxiv.org/html/2503.15921?utm_source=chatgpt.com)

---

## üß© [[vAttention]]
- **Summary**: Dynamic KV-cache memory manager without custom attention kernels.
- **Key Features**:
  - Handles dynamic cache sparsity.
  - Maintains compatibility with standard attention implementations.
- **Link**: [vAttention - arXiv](https://arxiv.org/html/2405.04437v1?utm_source=chatgpt.com)

---

## ‚öôÔ∏è [[CuAsmRL]]
- **Summary**: RL-based SASS optimizer for CUDA kernels.
- **Key Features**:
  - Uses deep RL to tune GPU assembly schedules.
  - Integrated into compiler pipelines.
  - Accelerates LLM specialized kernels by up to 1.5√ó.
- **Link**: [CuAsmRL - arXiv](https://arxiv.org/html/2501.08071v1?utm_source=chatgpt.com)

---

## üìå Notes
- These techniques focus on **low-level CUDA/kernel-level efficiency** for LLMs.
- Many are complementary and can be combined (e.g., FlashInfer + Speculative Decoding).

Great ‚Äî I‚Äôll begin compiling a detailed literature review that begins with a concise overview of general LLM optimization techniques, followed by an in-depth analysis of FlashAttention, FlashInfer, Speculative Decoding, and related methods. This will include historical evolution, current state-of-the-art techniques, code-level insights, and performance benchmarks across general LLMs. I‚Äôll prioritize arXiv papers and official blogs as sources. I‚Äôll let you know once the full review is ready.

# Optimization Techniques for Large Language Models: FlashAttention, FlashInfer, and Speculative Decoding

## Introduction and General Optimization Methods

Large Language Models (LLMs) deliver impressive results but come with high computational and memory costs, especially during inference. This has spurred a broad range of optimization techniques to make LLM training and serving more efficient. In general, common strategies include **model compression** methods and **efficient computation** methods:

- **Quantization:** Converting model weights (and sometimes activations) from 32-bit floats to lower precision (e.g. 8-bit or 4-bit integers) to reduce memory footprint and speed up matrix operations ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=Quantization%20converts%20the%20model%E2%80%99s%20weights,From%20the%20perspective%20of%20data)). Quantization can dramatically shrink model size while maintaining accuracy, as demonstrated by numerous recent works (GPTQ, AWQ, SmoothQuant, etc.) ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=Table%202%3A%20Existing%20generative%20LLM,LLM%C2%A0%5B118%5D%2C%20Chen%20et)).  
- **Pruning and Sparsity:** Removing redundant parameters or structure in the model (e.g. eliminating unimportant weights, neurons, or attention heads) to create a sparser model that computes faster. Techniques like SparseGPT and Wanda prune LLM weights with minimal loss ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=Sparsity%20%20%20Turbo%20Sparse%C2%A0,152)), and sparse attention patterns (BigBird, Longformer, etc.) restrict attention computation to a subset of tokens to reduce complexity ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=E,129)). Sparse models can leverage specialized hardware support to gain speedups.  
- **Knowledge Distillation:** Training a smaller ‚Äústudent‚Äù model to mimic the outputs of a large ‚Äúteacher‚Äù LLM. Distilled models (e.g. DistilBERT for BERT-sized models) aim to preserve most of the original capability but with far fewer parameters, enabling faster inference. This approach can yield compact models that achieve a significant fraction of the teacher‚Äôs accuracy at a fraction of the compute cost, though producing a high-quality distilled LLM for open-ended generation remains challenging in practice.  
- **Memory-Efficient Attention:** Because self-attention in Transformers scales quadratically with sequence length, many efforts target the attention mechanism. Early approaches like **approximate or sparse attention** (Reformer, Performer, BigBird, etc.) trade off some accuracy to reduce complexity, while *memory-efficient exact attention* implementations aim to compute standard attention faster by optimizing memory accesses ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=algorithms%20IO,any%20existing%20approximate%20attention%20method)) ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=kernel%20launching%20overhead.%20FlashAttention%C2%A0,kernels%20in%20a%20transformer%20block)). **FlashAttention**, discussed below, is a prime example that uses IO-aware tiling to reduce memory reads/writes and fuse the attention computation into a single high-performance kernel ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=memory,any%20existing%20approximate%20attention%20method)) ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=kernel%20launching%20overhead.%20FlashAttention%C2%A0,kernels%20in%20a%20transformer%20block)). This significantly cuts the memory overhead and runtime of attention.  
- **Inference Acceleration and Parallelism:** At inference time, additional techniques improve throughput and latency. These include **batched inference** and caching, where multiple input requests are processed together to amortize overhead, and efficient *KV cache* management to avoid recomputation of attention on previous tokens. Systems like **vLLM** introduce a ‚Äúpaged‚Äù attention memory manager to serve many queries efficiently by reusing cached keys/values ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=serving,0%20license)) ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=FlashInfer%20has%20been%20adopted%20by,or%20%2010%20creating%20an)). **Operator fusion** and optimized kernels (as in NVIDIA‚Äôs FasterTransformer, DeepSpeed-Inference, TensorRT-LLM, etc.) reduce framework overhead by merging multiple operations into one and using low-level GPU libraries ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=Operator%20Optimization%20%20%20FlashAttention%C2%A0,LLM%C2%A0%5B177%5D%2C%20CUTLASS%C2%A0%5B178%5D%2C%20ByteTransformer%C2%A0%5B179)). Finally, *parallel decoding algorithms* like speculative decoding (and related methods) explicitly reduce the sequential bottleneck of autoregressive generation, as we will explore in detail.

In the sections that follow, we provide a deeper literature review of three cutting-edge optimization techniques for LLMs ‚Äì **FlashAttention, FlashInfer, and Speculative Decoding** ‚Äì including their historical context, technical innovations, code-level implementations, performance benchmarks, and how they integrate with broader frameworks. We also highlight similar or complementary advances and discuss future directions.

## FlashAttention: Fast Memory-Efficient Attention

**Historical context and motivation:** *FlashAttention* was introduced by Dao et al. (2022) as an answer to the growing bottleneck of Transformer attention on long sequences ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=,analyze%20the%20IO%20complexity%20of)). Standard attention has quadratic time and memory complexity, which makes training or inferencing with long contexts extremely slow and memory-hungry. Prior attempts to alleviate this often involved approximate attention (trading accuracy for speed) or handcrafted sparse patterns. FlashAttention took a different approach: it reimagined the **exact** attention algorithm to be more hardware-efficient by focusing on I/O bottlenecks. The key insight was to make the attention computation **IO-aware** ‚Äì accounting for the cost of reads/writes between GPU high-bandwidth memory (HBM) and on-chip SRAM. By tiling the computations, FlashAttention reduces expensive memory traffic without altering the math of attention ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=algorithms%20IO,any%20existing%20approximate%20attention%20method)). In practical terms, FlashAttention fuses the multiple steps of attention (computing queries-keys dot products, applying softmax, and multiplying by values) into a single GPU kernel that processes chunks of the sequence at a time ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=kernel%20launching%20overhead.%20FlashAttention%C2%A0,kernels%20in%20a%20transformer%20block)). This fusion avoids materializing large intermediate attention matrices in global memory, dramatically cutting memory overhead and latency.

**Technical approach:** FlashAttention performs attention via block-wise tiling in shared memory. It splits the Q, K, V tensors into blocks and streams them through fast on-chip memory, so that each piece of the attention matrix is computed and immediately used, rather than writing out the entire NxN matrix. This design ensures far fewer HBM accesses than naive attention and keeps the GPU‚Äôs arithmetic units fed with data efficiently ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=algorithms%20IO,any%20existing%20approximate%20attention%20method)). The algorithm carefully balances work among thread blocks, and uses lower-level CUDA primitives (or custom Triton kernels) to maximize parallelism within each tile. As a result, the implementation attains very high hardware utilization. In fact, Dao et al. report that FlashAttention can reach up to 225 TFLOPs/sec on an NVIDIA A100 (which is ~72% of the theoretical peak) when training a model, yielding a **3‚Äì5√ó speedup** over the baseline HuggingFace attention implementation ([GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention#:~:text=loss%2C%20rotary%20embedding%29,don%27t%20need%20any%20activation%20checkpointing)). Importantly, these gains come **without** approximating the attention ‚Äî the outputs are identical to standard attention up to numerical precision.

**Evolution and state-of-the-art:** Following the original FlashAttention, the authors and others continued to refine the idea. **FlashAttention-2** (2023) extended the algorithm to better utilize GPU tensor cores and parallelize across sequence length, improving throughput especially on newer hardware ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=,matrices%2C%20thus%20improving%20the%20occupancy)) ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=FlashAttention,precision%20requires%20care%20to%20minimize)). FlashAttention-2 achieved around 2√ó speedups over the first version in various settings and addressed certain limitations in the original tiling scheme (e.g. better handling of long sequences with causal masks) ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=,matrices%2C%20thus%20improving%20the%20occupancy)). Most recently, **FlashAttention-3** (2024) introduced asynchronous execution and support for lower-precision arithmetic (FP8), pushing even closer to hardware limits ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=More%20fundamentally%2C%20FlashAttention,result%20of%20hardware%20specialization%20to)) ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=match%20at%20L710%20FlashAttention,maximum%20TFLOPs%2Fs%20on%20H100%20GPUs)). By overlapping the matrix multiplication and softmax phases (taking advantage of asynchrony in NVIDIA Hopper GPUs) and carefully managing numerical error, FlashAttention-3 achieves up to ~1.5√ó the speed of FlashAttention-2 and reaches **75%** of theoretical peak FLOPs on an H100 GPU ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=match%20at%20L710%20FlashAttention,maximum%20TFLOPs%2Fs%20on%20H100%20GPUs)). These iterations illustrate how the technique evolved to leverage new hardware features and further reduce latency.

**Performance benchmarks:** FlashAttention has demonstrated significant performance improvements in both training and inference contexts. In the original paper, it delivered **15% end-to-end training speedup** on BERT-large (sequence length 512) compared to the previous MLPerf record, and about **3√ó faster training** on GPT-2 with 1K sequence length ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=FlashAttention%20trains%20Transformers%20faster%20than,longer%20context%20in%20Transformers%2C%20yielding)). On longer sequence tasks (1K‚Äì4K tokens), it achieved ~2.4√ó speedups and even enabled transformers to handle sequences up to 16K or 64K with manageable overhead ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=FlashAttention%20trains%20Transformers%20faster%20than,256%20%28seq.%20length%2064K%2C%2063.1)). In terms of memory, FlashAttention‚Äôs in-place tiled computation means peak memory usage is much lower than standard attention, allowing larger batch sizes or longer contexts to fit in GPU memory. For inference specifically (where typically one processes one token at a time in auto-regressive decoding), FlashAttention‚Äôs authors later added an **inference mode optimization** (in FlashAttention 2.x) targeting the case of very short queries (e.g. a single token query attending to a long key/value cache) ([GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention#:~:text=2)). This further splits the KV cache loading across threads to maximize throughput when generating token-by-token. Overall, across various LLM architectures (GPT-style, encoders like BERT, etc.), FlashAttention provides robust speedups as long as the sequence length is moderate to long. The benefit is especially pronounced for longer inputs, where it can be multiple times faster than the default implementation while using less memory.

**Integration with frameworks:** FlashAttention has been widely adopted. The authors released an open-source library (`flash-attention`) with PyTorch bindings, which has been used in many research projects and even integrated into popular libraries. Notably, **PyTorch 2.2 (early 2024) incorporated FlashAttention-2 into its native `torch.nn.functional.scaled_dot_product_attention` API**, making the optimized kernel available by default for modern GPUs ([
    
      PyTorch 2.2: FlashAttention-v2 integration, AOTInductor | PyTorch
    
  ](https://pytorch.org/blog/pytorch2-2/#:~:text=,standardized%2C%20configurable%20logging%20mechanism%20called)) ([
    
      PyTorch 2.2: FlashAttention-v2 integration, AOTInductor | PyTorch
    
  ](https://pytorch.org/blog/pytorch2-2/#:~:text=torch.nn.functional.scaled_dot_product_attention%20%28SDPA%29%20now%20supports%20FlashAttention,maximum%20FLOPs%2Fs%20on%20A100%20GPUs)). This integration yields ~2√ó speedups over PyTorch‚Äôs earlier attention and achieves roughly 50‚Äì73% of theoretical peak FLOPs on A100 GPUs for attention operations ([
    
      PyTorch 2.2: FlashAttention-v2 integration, AOTInductor | PyTorch
    
  ](https://pytorch.org/blog/pytorch2-2/#:~:text=torch.nn.functional.scaled_dot_product_attention%20%28SDPA%29%20now%20supports%20FlashAttention,maximum%20FLOPs%2Fs%20on%20A100%20GPUs)). Other frameworks, such as Hugging Face Transformers and JAX-based libraries, have also provided support or recipes to use FlashAttention for faster training of transformer models. Because FlashAttention does not alter model architecture or require retraining, it is relatively easy to plug into existing models ‚Äì it purely replaces the low-level implementation of the attention forward/backward pass. This ease of integration has helped it become a *de facto* standard for efficient attention in new LLM training codebases. For instance, long-context models like Meta‚Äôs LLaMA-2 32K context version rely on FlashAttention-style kernels to manage the memory load. In summary, FlashAttention introduced a major step forward in exact attention efficiency, and its successive improvements (FlashAttn 2, 3) continue to define the state-of-the-art for high-performance attention. It set the stage for treating **attention as an I/O-bound operation** and demonstrated that significant speedups are achievable without sacrificing accuracy.

## FlashInfer: Optimized Kernels for LLM Serving

As LLMs moved from research to production, the inference workload ‚Äì especially serving user queries with minimal latency ‚Äì became a central focus. **FlashInfer** (Ye et al., 2024) emerged as a specialized library targeting *end-to-end LLM inference optimization*, with an emphasis on the self-attention bottleneck in decoder models. The motivation, as described by its authors, was that while dense matrix multiplication (GEMM) optimizations are well-studied, *less attention had been paid to optimizing the attention mechanism in the context of LLM *serving*** ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=efficacy%20of%20operators%20within%20Transformers,source%20library%20for)). In generation, the attention pattern differs from training: models use a *KV cache* of past keys and values, and each new token‚Äôs attention only covers the prior context (which grows over time). FlashInfer analyzes this process in three stages ‚Äì **prefill, decode, and append** ‚Äì to pinpoint bottlenecks and introduce tailored optimizations for each stage ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=During%20the%20prefill%20stage%2C%20attention,and%20queries%20of%20the%20appended)).

- *Prefill stage:* the initial stage where the model processes the user‚Äôs prompt (context) all at once, attending over the prompt tokens. Here attention is performed between the query and a freshly built (empty) KV cache of similar length.  
- *Decode stage:* the auto-regressive generation stage, where the model produces tokens one by one. For each new token, attention occurs between the single-token query and the accumulated KV cache of all previous tokens ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=During%20the%20prefill%20stage%2C%20attention,and%20queries%20of%20the%20appended)). This is a crucial loop where latency per token must be minimal.  
- *Append stage:* after a token is generated (or a batch of tokens in batched decoding), the new token‚Äôs key/value are **appended** to the KV cache for future use ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=During%20the%20prefill%20stage%2C%20attention,and%20queries%20of%20the%20appended)). Managing this efficiently (especially for batched requests of different lengths) is important for throughput.

**Technical innovations:** FlashInfer provides a **comprehensive suite of custom GPU kernels** covering all these stages with high efficiency. Its design is modular and *format-aware*: it supports multiple KV cache memory layouts (contiguous, ‚Äúragged‚Äù variable length, and even a paged layout used by systems like vLLM) ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=1,for%20long)). For each scenario, FlashInfer includes optimized kernels ‚Äì for example, a specialized kernel for the *decode* stage that assumes a single query token and streams through the KV cache with minimal overhead, and another for *prefill* that handles a full attention matrix. These kernels are implemented to be **batch-aware** as well: FlashInfer has variants for single-request inference versus batched inference (serving many prompts concurrently) ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=1,in%20an%20impressive%20up%20to)). A key contribution is how it handles *shared-prefix batching*: when multiple requests share a long common prefix (e.g. many users prompt with the same initial context), FlashInfer‚Äôs ‚Äúcascading‚Äù technique reuses the work across those requests to avoid redundant computations, yielding massive speedups in extreme cases ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=2.%20Optimized%20Shared,Notably)). In one benchmark, exploiting a shared 32k-token prompt for a batch of 256 requests, the FlashInfer approach was up to **31√ó faster** than a naive implementation (using vLLM‚Äôs paging attention as baseline) ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=2.%20Optimized%20Shared,Notably)).

Another innovation is support for **grouped-query attention and quantized keys/values**. Many modern LLMs employ *Grouped-Query Attention (GQA)* ‚Äì a variant where instead of each of N heads having separate key/value sets, heads are grouped to share keys and values (this reduces memory size of KV cache and bandwidth) ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation)). FlashInfer provides kernels optimized for GQA and for common quantization of KV caches (e.g. storing cache in 16-bit or 8-bit), including support for fused position encoding (RoPE) during attention computation ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation)). By optimizing memory access patterns for these compressed representations, FlashInfer can achieve **2‚Äì3√ó speedups** even when the KV cache is quantized or grouped, compared to standard implementations ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation)).

Under the hood, FlashInfer uses *just-in-time (JIT) code generation* to adapt its kernels to specific model shapes and deployment needs ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=present%20FlashInfer%3A%20a%20customizable%20and,end%20evaluations)). The library exposes a template where at compile-time you can specify the attention variant, data types, head count, etc., and it will JIT compile an optimized kernel specialized for that case ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=present%20FlashInfer%3A%20a%20customizable%20and,end%20evaluations)). This is done with a lightweight C++ template and CUDA code approach (the library is dependency-free and header-only for easy integration ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=FlashInfer%20has%20been%20developed%20by,FlashInfer%20has%20several%20unique%20advantages))). The result is that it can flexibly accommodate different model architectures or hardware specifics, similar in spirit to how *TVM or TensorRT* might tune an operator. Additionally, FlashInfer implements a custom **load-balanced scheduling** for batched decoding ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=format%20and%20composable%20formats%20to,art%20LLM%20serving)). In dynamic serving, some requests may finish earlier than others or have varying lengths, which can lead to load imbalance. FlashInfer‚Äôs scheduler tries to distribute work evenly across CUDA streams/blocks while remaining compatible with **CUDA Graphs** (which require static launch shapes) ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=format%20and%20composable%20formats%20to,art%20LLM%20serving)) ‚Äì a non-trivial balancing act.

**Performance benchmarks:** According to the authors‚Äô evaluation, FlashInfer delivers significant gains over existing inference solutions. At the kernel level, across diverse scenarios (single token decode, multi-token prefill, various batch sizes), it outperforms baseline kernels from vendor libraries and other open-source projects. End-to-end, when integrated into serving systems, FlashInfer achieved **29‚Äì69% lower inter-token latency** (the time per output token) compared to state-of-the-art compiler-based backends on a standard LLM serving benchmark ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=vLLM%20and%20MLC,LLM%20serving%20with%20parallel%20generation)). For long-context inference (e.g. prompts that are thousands of tokens long), it reduced latency by ~28‚Äì30% ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=across%20diverse%20inference%20scenarios%3A%20compared,LLM%20serving%20with%20parallel%20generation)), showing that its optimizations on memory access and batching pay off for extended sequences. In scenarios with *parallel generation* (serving multiple requests concurrently, leveraging batching), it offered around 13‚Äì17% throughput improvement versus the best alternatives ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=across%20diverse%20inference%20scenarios%3A%20compared,LLM%20serving%20with%20parallel%20generation)). These numbers indicate that FlashInfer doesn‚Äôt just optimize one narrow case but provides across-the-board efficiency improvements. Notably, FlashInfer‚Äôs gains are complementary to other techniques like quantization ‚Äì e.g., one can quantize an LLM and still use FlashInfer to accelerate the attention, stacking the benefits.

**Comparisons and adoption:** FlashInfer can be seen as part of a new wave of high-performance inference runtimes. Other frameworks like **DeepSpeed-Inference, FasterTransformer, and TensorRT-LLM** also provide optimized kernels (and support things like quantization and tensor parallelism). What sets FlashInfer apart is its focus and coverage of the *attention* operator specifically, with fine-grained optimizations for different caching patterns and batch behaviors. It essentially generalizes ‚ÄúFlashAttention-style‚Äù thinking to the whole inference process. The library has already been integrated with several systems: for example, the authors report that FlashInfer was adopted as the CUDA attention backend in **MLC-LLM** (an AI compiler stack for LLMs), and it‚Äôs used in the **vLLM** and **SGLang** serving frameworks ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=FlashInfer%20has%20been%20adopted%20by,or%20%2010%20creating%20an)). Because it offers both PyTorch APIs and a C++ API, developers can use it either for experimenting in Python or link it into custom serving infrastructure. In sum, FlashInfer represents the state-of-the-art in *specialized LLM inference kernels*, pushing latency down by addressing the unique characteristics of decoder attention workloads. It complements other optimizations (it can work alongside model quantization, weight pruning, etc.), and is likely to inspire further research into inference-specific kernel fusion and scheduling for Transformers.

## Speculative Decoding: Parallelizing Autoregressive Generation

While the above two techniques focus on lower-level efficiency (numerical precision, kernel fusion, memory access), **Speculative Decoding** attacks a different aspect of the LLM inference challenge: the *sequential nature* of text generation. In standard autoregressive decoding, an LLM generates text one token at a time, and each token‚Äôs computation depends on the previous token ‚Äì a inherently sequential process that limits throughput even on powerful hardware. Speculative decoding (sometimes called *speculative sampling*) is an algorithmic approach that allows multiple tokens to be generated per iteration by pairing a large model with a faster *draft model*. It was introduced in 2022 independently by researchers at Google and at DeepMind, drawing inspiration from CPU speculative execution in computer architecture ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,of%20serving%20the%20same%20model)) ([](https://arxiv.org/pdf/2302.01318#:~:text=We%20present%20speculative%20sampling%2C%20an,distribution%20of%20the%20target%20model)).

**Core idea:** Instead of having the large LLM (call it the ‚Äútarget model‚Äù) generate the next token every time, we use a **smaller model** (the ‚Äúdraft‚Äù or ‚Äúapproximation‚Äù model) to *speculatively generate k future tokens* in one go. Then the large model is used to **verify** those tokens in a single forward pass. Concretely, the process might work as follows ([Accelerating Large Language Model Inference: Techniques for Speed and Efficiency | by AI SageScribe | Mar, 2025 | Medium](https://medium.com/@aisagescribe/accelerating-large-language-model-inference-techniques-for-speed-and-efficiency-e5743bc3bbe5#:~:text=Speculative%20decoding%20is%20an%20optimization,The%20process%20involves)) ([](https://arxiv.org/pdf/2302.01318#:~:text=1,subset%20of%20the%20%F0%9D%90%BE%20draft)):

1. Use the small draft model to quickly generate a sequence of $k$ tokens continuing from the current context (this draft is done autoregressively by the small model, which is much faster per token).  
2. Feed those $k$ tokens into the large LLM in a single forward call to obtain the large model‚Äôs logits/probabilities for each of those positions. Compare the large model‚Äôs predicted tokens to the draft tokens.  
3. If the draft tokens match what the large model would produce (more precisely, if they are *accepted* under a certain criterion using a modified rejection sampling procedure), then we can accept some or all of them as correct and skip those steps ‚Äì effectively the large model has generated $k$ tokens with the cost of one forward pass ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,in%20the%20energy%20costs%20of)). If a token doesn‚Äôt match (the draft ‚Äúguessed wrong‚Äù too severely), the process falls back: the large model‚Äôs generation resumes from the last accepted token. In practice, one or a few tokens might be rejected and the large model might generate one token normally, then the speculative process can restart.

Crucially, the algorithm is designed so that **the output distribution is exactly the same as if the large model generated every token itself** ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,in%20the%20energy%20costs%20of)). This is achieved by the particular acceptance/rejection scheme (a form of rejection sampling) that ensures no statistical bias is introduced by the draft model‚Äôs guesses. In other words, speculative decoding doesn‚Äôt trade away any output quality or correctness ‚Äì it‚Äôs an *algorithmic speedup* that should give identical results as the original model, just faster on average.

**Performance gains:** Both Google and DeepMind reported significant speedups from speculative decoding. Google‚Äôs team (Leviathan et al.) demonstrated about **2√ó‚Äì3√ó reduction in inference time** for LLMs on tasks like translation and summarization, with identical output quality ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=application%20to%20translation%20and%20summarization,We%20have%20also%20seen%20speculative)). They highlighted that this translates into handling the same load with fewer machines (since each model instance produces tokens faster), which in turn saves on energy and cost ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,of%20serving%20the%20same%20model)). DeepMind‚Äôs version (Chen et al., 2023) similarly achieved around **2‚Äì2.5√ó speedup** in wall-clock time when generating with their 70B Chinchilla model in a distributed inference setup ([](https://arxiv.org/pdf/2302.01318#:~:text=with%20a%20novel%20modified%20rejection,modifications%20to%20the%20model%20itself)). What speedup is achieved depends on several factors: the ratio of the draft model‚Äôs speed to the large model, and the *acceptance rate* of draft tokens. In ideal conditions (a very fast draft model that guesses tokens the large model agrees with most of the time), speedups could approach the length of the draft $k$ (e.g. generating 4 tokens in one shot = 4√ó faster). In practice, 2‚Äì3√ó is a typical range reported for reasonably chosen drafts.

**Choosing a draft model:** A natural question is how to select or train the smaller model that provides the speculative suggestions. Intuitively, it should be much faster per token than the large model, but also reasonably predictive of the large model‚Äôs outputs so that it ‚Äúgets it right‚Äù often. Early experiments used, for example, a 6B parameter model as draft for a 70B model, or even smaller ratios like a 1.3B draft for a 13B model. A recent detailed study by Yan et al. (2024) evaluated speculative decoding on LLaMA and OPT models and found an interesting result: the draft model‚Äôs **latency** is more critical than its raw accuracy or size ([[2402.01528] Decoding Speculative Decoding](https://arxiv.org/abs/2402.01528#:~:text=speculative%20tokens%20and%20then%20uses,a%20new%20design%20space%20for)). In other words, a mediocre but extremely fast draft can yield better speedups than a slightly smarter but slower draft. They also observed that the draft model‚Äôs perplexity (a measure of language modeling capability) does **not** correlate strongly with speculative decoding performance ([[2402.01528] Decoding Speculative Decoding](https://arxiv.org/abs/2402.01528#:~:text=the%20draft%20model,higher%20throughput%20than)). This has led to exploring *hardware-efficient* draft models ‚Äì models that might be architecturally optimized for speed (even if they are not great general LLMs). In fact, Yan et al. designed a new draft model that achieved **111% higher throughput** than a standard draft (more than doubling tokens/sec) and still worked well with speculative decoding for LLaMA models ([[2402.01528] Decoding Speculative Decoding](https://arxiv.org/abs/2402.01528#:~:text=heavily%20on%20the%20latency%20of,further%20to%20all%20LLaMA%20models)). This suggests there is a lot of room to innovate in creating bespoke small models or strategies to maximize the benefit of speculative decoding.

**Implementations and integration:** Implementing speculative decoding requires coordinating two models, which adds some complexity to inference pipelines. In research code, it‚Äôs often done with custom scripts (calling the small model in a loop and then one call of the big model). There are open-source examples: e.g., a GitHub repository by **feifeibear** implements speculative sampling for HuggingFace Transformer models, supporting both Google‚Äôs and DeepMind‚Äôs algorithms ([GitHub - feifeibear/LLMSpeculativeSampling: Fast inference from large lauguage models via speculative decoding](https://github.com/feifeibear/LLMSpeculativeSampling#:~:text=Fast%20inference%20from%20transformers%20via,speculative%20decoding)) ([GitHub - feifeibear/LLMSpeculativeSampling: Fast inference from large lauguage models via speculative decoding](https://github.com/feifeibear/LLMSpeculativeSampling#:~:text=The%20speculative%20sampling%20is%20proposed,speculative%20sampling%3A%20Google%27s%20and%20Deepmind%27s)). This repo demonstrates usage with models like Bloom (7B as target, 560M as draft). Major frameworks are gradually adding support; for instance, the Hugging Face Transformers library has discussed an API for speculative decoding, and one can manually compose the two models in frameworks like PyTorch. The technique is model-agnostic as long as the draft and target share the same vocabulary (the draft essentially needs to predict tokens in the same token ID space). Some LLM serving systems have integrated speculative decoding in custom ways ‚Äì **Google** has reportedly deployed it in certain internal products by 2024, seeing ‚Äúremarkable speed-ups in inference while maintaining the same quality‚Äù ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=application%20to%20translation%20and%20summarization,We%20have%20also%20seen%20speculative)). On the academic side, researchers have also combined speculative decoding with *batch scheduling*: Liu et al. (2024) point out that if many requests are batched, naive speculative decoding might actually hurt latency under heavy load (especially if the draft model‚Äôs accuracy is low) ([[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066#:~:text=techniques,hence%20the%20associated%20speculative)) ([[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066#:~:text=which%20are%20then%20verified%20by,called%20goodput%2C%20which%20characterizes%20the)). They propose **SmartSpec**, a dynamic scheme that decides per request how many tokens to speculate (including the option of not using speculation for that step) based on current system load and the observed accuracy of the draft model ([[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066#:~:text=no%20best%20speculation%20length%20work,of%20speculative%20decoding%2C%20including%20traditional)). By dynamically adjusting the speculative length, their method avoided the cases where speculation overhead would outweigh benefits, yielding consistent improvements ‚Äì up to **3.2√ó lower latency** than no speculation even in high-load scenarios ([[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066#:~:text=execution%20costs%20,style%20decoding)).

**Recent advances and variants:** Speculative decoding has sparked a line of follow-up innovations. One branch of work aims to eliminate the need for a separate draft model by enhancing the main model itself to generate multiple tokens in one step. For example, **Medusa** (Cai et al., 2023) adds *multiple decoding heads* to the transformer, each head predicting a different next token in parallel ([[2401.10774] ‚ñ†("")Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://ar5iv.org/pdf/2401.10774#:~:text=their%20implementation%20is%20impeded%20by,number%20of%20decoding%20steps%20required)). These extra heads are trained (via fine-tuning) to produce plausible continuations, which the model then verifies internally using a tree-like attention mechanism. Medusa achieved up to **2.3‚Äì3.6√ó speedups** with its best variant, without requiring any external draft model ([[2401.10774] ‚ñ†("")Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://ar5iv.org/pdf/2401.10774#:~:text=We%20evaluate%20Medusa%20on%20models,this%20implementation%20is%20available%20at)). This ‚Äúsingle-model speculative decoding‚Äù approach shows that even the large model can be augmented to speculate on its own future tokens, given some additional training. Another work, **ParallelSpec** (Xiao et al., 2024), trains a *parallel drafter model* that can generate a block of $k$ tokens in one forward pass (breaking the draft model‚Äôs own autoregressive loop) ([ParallelSpec: Parallel Drafter for Efficient Speculative Decoding | OpenReview](https://openreview.net/forum?id=SXvb8PS4Ud#:~:text=computational%20burden%20in%20speculative%20decoding,in%20latency%20up%20to%2062)). ParallelSpec can be plugged into a normal speculative decoding framework and was shown to further improve latency ‚Äì e.g. it sped up an existing speculative method (EAGLE) by an extra 9‚Äì17%, and combined with that method achieved an overall **2.84√ó speedup on Llama-2-13B** ([ParallelSpec: Parallel Drafter for Efficient Speculative Decoding | OpenReview](https://openreview.net/forum?id=SXvb8PS4Ud#:~:text=speculative%20model,party%20evaluation%20criteria)). The idea of predicting multiple tokens in parallel is also related to a technique called **lookahead or n-step decoding** (where the model tries to lookahead n tokens, sometimes used in non-speculative contexts as well) ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=Sharda%20et%20al.%C2%A0,215%2C%20158)).

Yet another variant is **Speculative Beam Search or Speculative Branching**, as explored by Together AI‚Äôs **SpecExec** and others, where multiple speculative drafts are explored in parallel and the first to validate is taken (somewhat akin to branch prediction in CPUs). These methods aim to improve the *worst-case* latency by not relying on a single speculative path. While these are active research directions, they all share the common thread of attempting to get ‚Äútwo or three tokens for the price of one‚Äù ([Speculative decoding: cost-effective AI inferencing - IBM Research](https://research.ibm.com/blog/speculative-decoding#:~:text=In%20speculative%20decoding%2C%20the%20forward,has%20validated%20to%20that%20point)) in LLM generation. IBM Research recently demonstrated a system combining speculative decoding with their **paged attention** infrastructure (for high-throughput serving) and managed to *halve the latency and quadruple the throughput* of a production 20B-code model, by carefully marrying these techniques ([Speculative decoding: cost-effective AI inferencing - IBM Research](https://research.ibm.com/blog/speculative-decoding#:~:text=experience)) ([Speculative decoding: cost-effective AI inferencing - IBM Research](https://research.ibm.com/blog/speculative-decoding#:~:text=The%20speculation%20can%20be%20done,or%20tripling%20its%20inferencing%20speed)). This shows speculative decoding can be combined with memory optimizations and batching to compound efficiency gains. One challenge they noted is that naive speculative decoding can reduce latency but *also reduce throughput* if not done carefully (because the draft model work is extra overhead that doesn‚Äôt increase token output if used serially) ([Speculative decoding: cost-effective AI inferencing - IBM Research](https://research.ibm.com/blog/speculative-decoding#:~:text=experience)). By overlapping the draft model with other requests and using efficient batching (the ‚Äúpaged‚Äù KV cache system), they achieved both lower latency and higher throughput ([Speculative decoding: cost-effective AI inferencing - IBM Research](https://research.ibm.com/blog/speculative-decoding#:~:text=hosting%20the%20model,half%20while%20quadrupling%20its%20throughput)).

**Quality and limitations:** A critical aspect of speculative decoding is that it maintains output quality by design (no degradation in perplexity or fidelity since the large model ultimately vets the output). However, it does introduce complexity and a bit of overhead ‚Äì if the draft model‚Äôs suggestions are frequently rejected, time is wasted. Thus, if the draft model is poorly chosen, speculative decoding could even slow down inference. The research community has addressed this by developing formulas for expected speedup given draft accuracy and by devising adaptive strategies (like SmartSpec) to disable or shorten speculative generation when it‚Äôs not beneficial ([[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066#:~:text=which%20are%20then%20verified%20by,called%20goodput%2C%20which%20characterizes%20the)) ([[2406.14066] Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066#:~:text=dynamically%20determines%20the%20best%20speculation,of%20speculative%20decoding%2C%20including%20traditional)). In summary, speculative decoding stands out as an *algorithmic acceleration* that leverages extra compute (a smaller model‚Äôs compute) to reduce the serial work of a large model. It has proven effective in practice, delivering 2‚Äì3√ó throughput gains in multiple studies ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,in%20the%20energy%20costs%20of)) ([](https://arxiv.org/pdf/2302.01318#:~:text=with%20a%20novel%20modified%20rejection,modifications%20to%20the%20model%20itself)), and is influencing the future design of both model architectures and inference systems. As LLMs continue to be deployed in latency-critical settings, techniques like this are becoming increasingly important.

## Related Techniques and Future Directions

The landscape of LLM optimization is rapidly evolving, and FlashAttention, FlashInfer, and Speculative Decoding are part of a broader ecosystem of techniques that often complement each other. We briefly highlight some related advances and anticipated future innovations:

- **Multi-Query and Grouped-Query Attention:** A straightforward way to cut memory and compute in decoder models is to reduce the number of distinct key/value sets. In **Multi-Query Attention (MQA)**, all attention heads (or groups of heads) share a single K and V matrix instead of each head having its own ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation)). This was used in models like PaLM to great effect, as it reduces KV cache size by the number of heads (e.g. 8√ó or more) with only minor drops in quality. FlashInfer‚Äôs support for Grouped-Query attention kernels ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation)) reflects how important this is in practical LLM serving ‚Äì it optimizes the memory bandwidth when reading the cache, which is often a bottleneck in generation. Future models may adopt MQA by default to make long-context handling more tractable, and toolsets will continue to optimize for it.

- **System-Level Scheduling and Parallelism:** Beyond kernel-level optimizations, we expect more intelligent scheduling of LLM inference on hardware. Approaches like **continuous batching** (as implemented in vLLM) dynamically group incoming requests to maximize GPU utilization, and methods like **prefetching** the next tokens‚Äô computation while the current token is being used (overlap of compute and communication) can hide latency. The idea of using *asynchronous execution* which FlashAttention-3 introduced at the kernel level ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=More%20fundamentally%2C%20FlashAttention,result%20of%20hardware%20specialization%20to)) might expand to the system level: e.g., overlapping an LLM‚Äôs computation for token $t+1$ with some finishing work of token $t$ where possible.

- **Lower Precision and Quantization at Runtime:** While weight quantization to 4-bit is common for model size reduction, *activations* during inference are often kept in higher precision. New hardware (like NVIDIA Hopper GPUs) supports **FP8** arithmetic, and research on 8-bit or 4-bit activations is underway. FlashAttention-3 showed one example, using FP8 accumulation with careful error correction to boost speed ([](https://tridao.me/publications/flash3/flash3.pdf#:~:text=achieves%20close%20to%201,FP8%20is%20competitive2%20with%20a)). In the future, we might see **entire forward passes in 8-bit** for certain models, especially if combined with calibration techniques to maintain quality. Runtime quantization (quantizing on the fly) or mixing precisions could offer additional latency gains.

- **Sparse and Structured Pruning:** While some structured sparsity (like 2:4 sparsity on NVIDIA Ampere GPUs) is already supported by hardware, LLMs currently don‚Äôt widely use sparse weights at inference. This could change as techniques like **SparseGPT** (post-training weight pruning with minimal loss) have shown the feasibility of pruning 50% or more of weights in GPT-style models with little impact on perplexity. If hardware libraries start to support unstructured sparsity efficiently (e.g., via compressed sparse row matrix multiply), deploying pruned LLMs could yield direct speedups proportional to sparsity. Research is also moving toward *learned sparsity patterns* (pruning entire heads or MLP neurons that contribute least). A complementary idea is **mixture-of-experts (MoE)** models, which are large but activate only a small subset of weights per token ‚Äì effectively a form of dynamic sparsity at runtime. MoEs can reduce the per-token compute, but they add complexity in routing to the correct ‚Äúexpert.‚Äù Future systems might incorporate some lightweight expert routing to skip unnecessary computations for certain inputs.

- **Optimized Transformers and Alternatives:** On the algorithmic side, there is interest in replacing or modifying the Transformer architecture itself to be more inference-efficient. For example, **Linformers, Performer, or other linear attention** approximations could allow longer contexts by making attention $O(N)$, though so far they often come at a quality cost or only apply to encoders. **Retrieval-based models** (like Retro, RAG) alleviate the need for huge context windows by offloading to a database lookup, thus potentially reducing how much the model needs to hold in memory or process per token. Additionally, some research explores *sequence parallelism* (splitting the sequence among devices) for faster generation of extremely long outputs, and even non-autoregressive generation for certain tasks (though for open-ended text, non-AR models haven‚Äôt rivaled AR LLMs yet in quality). Each of these directions could impact how future LLMs are optimized: we might see hybrids that retrieve text to avoid recomputation, or adaptive computation time models that spend less effort on ‚Äúeasy‚Äù tokens and more on ‚Äúhard‚Äù tokens.

- **Combining Strategies:** A salient trend is that many of these techniques can be layered together. For example, a modern LLM deployment might use 4-bit quantization for weights, FlashAttention kernels for attention, *and* speculative decoding for generation. These are not mutually exclusive ‚Äì in fact, their benefits multiply. One must be careful in combining them (as IBM noted, speculative decoding and certain caching strategies required careful integration to avoid trade-offs ([Speculative decoding: cost-effective AI inferencing - IBM Research](https://research.ibm.com/blog/speculative-decoding#:~:text=But%20there%E2%80%99s%20a%20hitch,half%20while%20quadrupling%20its%20throughput))), but the end goal is to push the efficiency envelope from all angles. We anticipate more holistic approaches where a serving system like an ‚ÄúLLM compiler‚Äù automatically applies a suite of optimizations (quantize weights, choose an optimal draft model for spec decoding, select the best attention kernel, etc.) given a target model and hardware. Projects like Hugging Face‚Äôs Optimum and OpenAI‚Äôs research into optimized model cascades are steps in this direction.

**Conclusion:** Optimization techniques for LLMs have advanced rapidly, turning what used to be infeasible (like running a 100B model in real-time) into reality. **FlashAttention** tackled the fundamental computation of attention, cutting training times and enabling longer contexts without sacrificing fidelity ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=FlashAttention%20trains%20Transformers%20faster%20than,256%20%28seq.%20length%2064K%2C%2063.1)). **FlashInfer** built on that spirit to address end-to-end serving efficiency, accounting for caching and batch dynamics to drive down latency in production settings ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=vLLM%20and%20MLC,LLM%20serving%20with%20parallel%20generation)). **Speculative Decoding** approached the problem from an algorithmic angle, showing we can use an auxiliary model (or new architectures) to leap over the sequential generation limit and achieve multi-token-per-step generation ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,in%20the%20energy%20costs%20of)) ([](https://arxiv.org/pdf/2302.01318#:~:text=with%20a%20novel%20modified%20rejection,modifications%20to%20the%20model%20itself)). Each of these techniques has influenced industry practice and opened new research questions. Going forward, we expect these lines of work to converge into even more powerful solutions ‚Äì perhaps LLMs with built-in speculative generation heads running on specialized hardware with minimal precision, all orchestrated by intelligent schedulers. The literature reviewed here represents the state-of-the-art as of 2024‚Äì2025, but continuous innovations in both algorithms and hardware will undoubtedly shape the next generation of large language model deployment and make them more efficient and accessible than ever. 

**Sources:** The content above is based on a synthesis of recent research papers, arXiv preprints, and technical blogs, including the FlashAttention series ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=kernel%20launching%20overhead.%20FlashAttention%C2%A0,kernels%20in%20a%20transformer%20block)) ([[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135#:~:text=memory,any%20existing%20approximate%20attention%20method)) ([GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention#:~:text=loss%2C%20rotary%20embedding%29,don%27t%20need%20any%20activation%20checkpointing)), the FlashInfer paper and blog ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=1,for%20long)) ([Accelerating Self-Attentions for LLM Serving with FlashInfer | FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html#:~:text=post%20%20for%20more%20details,H100%2C%20compared%20to%20vLLM%20implementation)) ([[2501.01005] 1 Introduction](https://ar5iv.org/abs/2501.01005#:~:text=vLLM%20and%20MLC,LLM%20serving%20with%20parallel%20generation)), and multiple works on speculative decoding ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=In%202022%20we%20published%20,in%20the%20energy%20costs%20of)) ([Looking back at speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/#:~:text=application%20to%20translation%20and%20summarization,We%20have%20also%20seen%20speculative)) ([](https://arxiv.org/pdf/2302.01318#:~:text=with%20a%20novel%20modified%20rejection,modifications%20to%20the%20model%20itself)) ([[2402.01528] Decoding Speculative Decoding](https://arxiv.org/abs/2402.01528#:~:text=speculative%20tokens%20and%20then%20uses,a%20new%20design%20space%20for)), as well as survey articles on LLM inference efficiency ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=optimization%20methods%20include%20quantization%2C%20sparsity%2C,a%20qualified%20and%20quantitative%20comparison)) ([Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective](https://arxiv.org/html/2410.04466v1#:~:text=Operator%20Optimization%20%20%20FlashAttention%C2%A0,LLM%C2%A0%5B177%5D%2C%20CUTLASS%C2%A0%5B178%5D%2C%20ByteTransformer%C2%A0%5B179)). These sources provide detailed benchmarks and explanations for the described techniques.
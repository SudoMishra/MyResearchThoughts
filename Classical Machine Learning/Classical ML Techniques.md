# ðŸ¤– Classical Machine Learning Techniques â€“ Reference Table

A comprehensive index of core ML algorithms and models. Each technique is linked to a detailed note using Obsidian-style `[[Backlinks]]`.

| **Technique**                   | **Primary Use Case / Description**                                                   |
|----------------------------------|----------------------------------------------------------------------------------------|
| [[Linear Regression]]           | Predict continuous outcomes using a linear combination of input features               |
| [[Logistic Regression]]         | Predict binary outcomes; outputs probabilities via sigmoid function                    |
| [[Decision Tree]]               | Tree-based classifier/regressor using feature splits (high interpretability)           |
| [[Random Forest]]               | Ensemble of decision trees using bagging to improve generalization                     |
| [[Gradient Boosting]]           | Boosting ensemble that sequentially improves weak learners (e.g., XGBoost, LightGBM)   |
| [[Naive Bayes]]                 | Probabilistic classifier using Bayesâ€™ Theorem with feature independence assumption     |
| [[K-Nearest Neighbors (KNN)]]   | Instance-based model that classifies based on proximity to training data points        |
| [[Support Vector Machine (SVM)]]| Finds optimal hyperplane for classification; supports non-linear kernels               |
| [[K-Means Clustering]]          | Unsupervised clustering based on minimizing intra-cluster variance                     |
| [[Hierarchical Clustering]]     | Builds a tree of clusters (dendrogram) using agglomerative or divisive methods         |
| [[Principal Component Analysis (PCA)]] | Dimensionality reduction via orthogonal transformation                         |
| [[Linear Discriminant Analysis (LDA)]] | Supervised dimensionality reduction for maximizing class separability         |
| [[DBSCAN]]                      | Density-based clustering method robust to noise and outliers                           |
| [[Gaussian Mixture Models (GMM)]] | Probabilistic clustering using soft assignments and EM algorithm                    |
| [[Ridge Regression]]            | Linear regression with L2 regularization to prevent overfitting                        |
| [[Lasso Regression]]            | Linear regression with L1 regularization for feature selection                         |
| [[Elastic Net]]                 | Combines L1 and L2 penalties for regression                                            |
| [[Isolation Forest]]            | Tree-based anomaly detection technique                                                 |
| [[One-Class SVM]]               | Detect outliers by learning a decision boundary around normal data                     |
| [[AdaBoost]]                    | Boosting algorithm that focuses on hard-to-classify instances                          |

